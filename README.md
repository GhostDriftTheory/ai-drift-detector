Certificate-Based Drift Detection Audit for Time-Series Forecasting (Electricity Demand Ã— Weather)

Keywords: drift detection, time-series forecasting, model monitoring, MLOps, audit trail, reproducibility, accountability, electricity demand forecasting

<img src="certificate-drift-audit-flow.png" width="600" alt="Certificateâ€“Ledgerâ€“Verifier Flow">

Protocol overview: fixed certificate â†’ append-only ledger â†’ independent verifier (OK / NG).

ai-drift-detector (Ghost Drift Audit v9.9) is a certificate-based audit engine designed to prioritize validity and accountability over mere statistical accuracy. Unlike conventional monitoring that relies on post-hoc threshold tuning, this engine outputs a verifiable certificate and an immutable ledger, allowing any third party to reproduce the exact audit verdict from the same inputs.

Note: Bundled CSVs are reproducibility datasets provided to verify the audit protocol. The system is designed with strict data binding; it will cease execution if the input integrity or logic identity does not match the predefined fingerprints.

---

ğŸ”— Quick Links

* ğŸ“‚ **Source Code:** [GitHub Repository](https://github.com/GhostDriftTheory/ai-drift-detector)
* ğŸ“œ **Main Script:** [ghost_drift_audit_EN.py](https://github.com/GhostDriftTheory/ai-drift-detector/blob/main/ai-drift-detector.py)
* ğŸ“¦ **Download:** [Project ZIP](https://github.com/GhostDriftTheory/ai-drift-detector/archive/refs/heads/main.zip)
* ğŸ“– **Documentation:** [Online Manual](https://ghostdrifttheory.github.io/ai-drift-detector/) ([âš™ï¸ Jump to Execution Mode](https://ghostdrifttheory.github.io/ai-drift-detector/#profile))
* ğŸš¨ **Support:** [Report Issues](https://github.com/GhostDriftTheory/ai-drift-detector/issues)

---

## ğŸ“‘ Audit Report (PDF)

- **Report:** [Scientific Audit Report on Structural Integrity of Forecasting Models](./Scientific%20Audit%20Report%20on%20Structural%20Integrity%20of%20Forecasting%20Models.pdf)
- **Verdict:** NG (TAU_CAP_HIT)
- **Protocol:** Ghost Drift Audit v8.0

---

## ğŸ’ Design Philosophy: From "Probabilistic" to "Accountable"

To address the â€œopaque inferenceâ€ problem in conventional AI operations, this framework provides:

> [!TIP]
> **Audit-First Design**  
> Alongside running predictions, it automatically generates objectively verifiable **evidence** for third parties.

> [!IMPORTANT]
> **Tamper-Evident Fingerprints**  
> It fixes hash fingerprints of input data and configuration parameters, making post-hoc modifications mathematically detectable.

> [!NOTE]
> **Verifiable Integrity**  
> Rather than mere statistical optimality, it makes visible the modelâ€™s **faithful adherence** to operational rules.

---

## ğŸ›  Technical Specifications

### System Requirements

- **Language:** Python 3.10+
- **Dependencies:** numpy, pandas, matplotlib, lightgbm

### Project Structure

```text
.
â”œâ”€â”€ ghost_drift_audit_JP.py    # Core Logic & Audit Engine
â”œâ”€â”€ electric_load_weather.csv  # Input: Weather (dummy for smoke test)
â”œâ”€â”€ power_usage.csv            # Input: Demand (dummy for smoke test)
â””â”€â”€ adic_out/                  # Output: Accountability Ledger

âš™ï¸ Execution Profiles
Switch the strictness of the audit via the configuration settings in ai-drift-detector.py.
ãƒ»demo (Strictness: Low)
ã€€ãƒ»Target: Protocol verification / learning.
ã€€ãƒ»Key Features: Prioritizes understanding audit flow and evidence output.
ãƒ»paper (Strictness: Mid)
ã€€ãƒ»Target: Research / reproducible experiments.
ã€€ãƒ»Key Features: Ensures computational reproducibility via fixed seeds.
ãƒ»commercial (Strictness: High)
ã€€ãƒ»Target: Production / High-stakes decision-making.
ã€€ãƒ»Key Features: Produces strict gate checks including Logic/Source Identity verification.
# Configuration within ai-drift-detector.py
# v9.9 is pre-configured to handle Logic Identity and BOM Resilience.
STRICT_AUDIT_MODE = True 

ğŸš€ Deployment & Usage
1. Setup
pip install numpy pandas matplotlib

2. Data Preparation
Place the power_usage.csv and electric_load_weather.csv files in the same directory as the .py script.
[!CAUTION]
No Synthetic Fallback: The v9.9 engine prohibits falling back to dummy data in Strict Mode. Use the provided reproducibility datasets or your own audited datasets with valid headers.

3. Run
python ai-drift-detector.py

4. Verification (Outputs)

ğŸ“œ audit_record.json: The Certificate. A JSON snapshot of execution conditions and logic fingerprints.
ğŸ“‘ audit_log.jsonl: The Ledger. An append-only hash chain recording the full processing history.
ğŸ“¦ audit_bundle.zip: A self-contained package for independent verification.
âš–ï¸ Scope & Integrity (Non-claims)
ğŸ¯ Scope & Limits

Scope: Provides a mathematical framework (including FejÃ©râ€“Yukawa kernel approaches) to make model behavior and structural shifts observable and verifiable.
Non-claims: Does not guarantee zero future error or absolute "truth"; it guarantees the reproducibility of the audit process itself.

ğŸ›¡ï¸ Threat Model (Tamper Detection)

Threshold manipulation: Detected via Certificate mismatch and signed Cap records.
Logic manipulation: Detected via Logic Identity Proxy hash change.
Data fabrication: Detected via Source Identity (SHA-256) fingerprinting.

ğŸ“œ License & Acknowledgments

Code: MIT License
Reproducibility Data: Included for protocol verification.

Patent Notice:
This repository implements techniques related to a pending patent application.
Japanese Patent Application No. ç‰¹é¡˜2025-182213.
This notice does not restrict use of the open-source code under the MIT License.

From â€œpredictionâ€ to â€œaccountability.â€
This repository provides a practical reference implementation for certificate-based drift detection and accountable model monitoring.
Produced by GhostDrift Mathematical Institute (GMI) â€” Official Website | Online Documentation
